{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d216e91",
   "metadata": {},
   "source": [
    "# Predicting Housing Prices with Regularized Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec1e00b",
   "metadata": {},
   "source": [
    "# 1. Data Preparation:\n",
    "\n",
    "a. Load the dataset using pandas.\n",
    "b. Explore and clean the data. Handle missing values and outliers.\n",
    "c. Split the dataset into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a72f263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('Housing.csv')\n",
    "\n",
    "# Define your features (X) and target vaiable (y)\n",
    "X = df[['area', 'bedrooms', 'bathrooms', 'stories']]  \n",
    "y = df['price']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a0315a",
   "metadata": {},
   "source": [
    "# 2. Implement Lasso Regression:\n",
    "\n",
    "a. Choose a set of features (independent variables, X) and house prices as the dependent variable (y).\n",
    "b. Implement Lasso regression using scikit-learn to predict house prices based on the selected features.\n",
    "c. Discuss the impact of L1 regularization on feature selection and coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0d0c80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_error\n",
    "\n",
    "# Create a Lasso regression model\n",
    "lasso = Lasso(alpha=0.01)  # You can adjust the alpha (penalty parameter) for regularization\n",
    "\n",
    "# Fit the model to the training data\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = lasso.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8123e2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted House Prices (Lasso Model):\n",
      "Prediction 1: $4954326.83\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a trained Lasso model (lasso) and new data in a DataFrame (new_data)\n",
    "new_data = pd.DataFrame({\n",
    "    'area': [6000],\n",
    "    'bedrooms': [4],\n",
    "    'bathrooms': [1],\n",
    "    'stories': [2],\n",
    "    # Add values for other features\n",
    "})\n",
    "\n",
    "# Make predictions for new data\n",
    "predicted_prices_lasso = lasso.predict(new_data)\n",
    "\n",
    "print(\"Predicted House Prices (Lasso Model):\")\n",
    "for i, price in enumerate(predicted_prices_lasso):\n",
    "    print(f\"Prediction {i+1}: ${price:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722a3e4e",
   "metadata": {},
   "source": [
    "# 3. Evaluate the Lasso Regression Model:\n",
    "\n",
    "a. Calculate the Mean Absolute Error (MAE), Mean Squared Error (MSE), and Root Mean Squared Error (RMSE) for the Lasso regression model.\n",
    "b. Discuss how the Lasso model helps prevent overfitting and reduces the impact of irrelevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85343d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 1158970.480386592\n",
      "Mean Squared Error: 2457741643358.91\n",
      "Root Mean Squared Error: 1567718.6110265164\n"
     ]
    }
   ],
   "source": [
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347b674f",
   "metadata": {},
   "source": [
    "# 4. Implement Ridge Regression:\n",
    "\n",
    "a. Select the same set of features as independent variables (X) and house prices as the dependent variable (y).\n",
    "b. Implement Ridge regression using scikit-learn to predict house prices based on the selected features.\n",
    "c. Explain how 12 regularization in Ridge regression differs from L1 regularization in Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1525a27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Create a Ridge regression model\n",
    "ridge = Ridge(alpha=1.0)  # You can adjust the alpha (penalty parameter) for regularization\n",
    "\n",
    "# Fit the model to the training data\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_ridge = ridge.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdd2ba35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted House Prices:\n",
      "Prediction 1: $4961460.04\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a trained Ridge model (ridge) and new data in a DataFrame (new_data)\n",
    "new_data = pd.DataFrame({\n",
    "    'area': [6000],\n",
    "    'bedrooms': [4],\n",
    "    'bathrooms': [1],\n",
    "    'stories': [2],\n",
    "    # Add values for other features\n",
    "})\n",
    "\n",
    "# Make predictions for new data\n",
    "predicted_prices = ridge.predict(new_data)\n",
    "\n",
    "print(\"Predicted House Prices:\")\n",
    "for i, price in enumerate(predicted_prices):\n",
    "    print(f\"Prediction {i+1}: ${price:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e35ab5d",
   "metadata": {},
   "source": [
    "# 5. Evaluate the Ridge Regression Model:\n",
    "\n",
    "a. Calculate the MAE, MSE, and RMSE for the Ridge regression model.\n",
    "b. Discuss the benefits of Ridge regression in handling multicollinearity among features and its impact on the model's coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "368419c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression Metrics:\n",
      "Mean Absolute Error: 1158471.4534767317\n",
      "Mean Squared Error: 2456765538413.524\n",
      "Root Mean Squared Error: 1567407.2662883517\n"
     ]
    }
   ],
   "source": [
    "# Calculate evaluation metrics\n",
    "mae_ridge = mean_absolute_error(y_test, y_pred_ridge)\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "rmse_ridge = mean_squared_error(y_test, y_pred_ridge, squared=False)\n",
    "\n",
    "print(\"Ridge Regression Metrics:\")\n",
    "print(\"Mean Absolute Error:\", mae_ridge)\n",
    "print(\"Mean Squared Error:\", mse_ridge)\n",
    "print(\"Root Mean Squared Error:\", rmse_ridge)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acda95b",
   "metadata": {},
   "source": [
    "# 6. Model Comparison:\n",
    "\n",
    "a. Compare the results of the Lasso and Ridge regression models.\n",
    "b. Discuss when it is preferable to use Lasso, Ridge, or plain linear regression."
   ]
  },
  {
   "cell_type": "raw",
   "id": "82746b52",
   "metadata": {},
   "source": [
    "Lasso is preferable when you suspect that some features are irrelevant, and you want feature selection. It can be a better choice when you have a high-dimensional dataset and want a simpler, more interpretable model.\n",
    "\n",
    "Ridge is useful when dealing with multicollinearity, as it stabilizes coefficient estimates and prevents overfitting. It's a good choice when all features are potentially relevant but you want to avoid high sensitivity to feature variations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4028eb0a",
   "metadata": {},
   "source": [
    "# 7. Hyperparameter Tuning:\n",
    "\n",
    "a. Explore hyperparameter tuning for Lasso and Ridge, such as the strength of regularization, and discuss how different hyperparameters affect the models."
   ]
  },
  {
   "cell_type": "raw",
   "id": "08d12d0f",
   "metadata": {},
   "source": [
    "For both Lasso and Ridge, the alpha parameter controls the strength of regularization. \n",
    "A lower alpha value reduces regularization, allowing the model to fit the training data more closely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d546c5ec",
   "metadata": {},
   "source": [
    "# 8. Model Improvement:\n",
    "\n",
    "a. Investigate any feature engineering or data preprocessing techniques that can enhance the performance of the regularized regression models."
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ac012c4",
   "metadata": {},
   "source": [
    "Feature engineering can include creating new features, scaling or transforming existing features, and handling categorical variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772798e4",
   "metadata": {},
   "source": [
    "# 9. Conclusion:\n",
    "\n",
    "a. Summarize the findings and provide insights into how Lasso and Ridge regression can be valuable tools for estimating house prices and handling complex datasets."
   ]
  },
  {
   "cell_type": "raw",
   "id": "9250689c",
   "metadata": {},
   "source": [
    "Regularized regression techniques like Lasso and Ridge are effective in real estate prediction as they not only provide predictive models but also handle issues like multicollinearity, feature selection, and overfitting. The choice between Lasso and Ridge depends on the specific characteristics of the dataset and the desired trade-off between model complexity and predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761ee2d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f74c5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8416541c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87c0c621",
   "metadata": {},
   "source": [
    "# Diagnosing and Remedying Heteroscedasticity and Multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b741ce7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
