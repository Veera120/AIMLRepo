{
 "cells": [
  {
   "cell_type": "raw",
   "id": "55c17af6",
   "metadata": {},
   "source": [
    "You are part of a team developing a text classification system for a news aggregator platform. The platform aims to categorize news articles into different topics automatically. The dataset contains news articles along with their corresponding topics. Perform only the Feature extraction techniques.\n",
    "\n",
    "Dataset Link: https://www.kaggle.com/datasets/therohk/million-headlines\n",
    "\n",
    "Data Exploration: Begin by exploring the dataset. What are the different topics/categories present in the dataset? What is the distribution of articles across these topics?\n",
    "\n",
    "Bag-of-Words (BoW): Implement a Bag-of-Words (BoW) model using Count Vectorizer or TF-IDF to transform the text data into numerical features. Discuss the advantages and limitations of Bow in this context. Apply both unigram and bigram techniques and compare their effects on classification accuracy.\n",
    "\n",
    "N-grams: Explore the use of N-grams (bi-grams, tri-grams) in feature engineering. How do different N-gram ranges impact the performance of the classification model?\n",
    "\n",
    "TF-IDF: Apply TF-IDF (Term Frequency-Inverse Document Frequency) to the text data. Describe how TF-IDF works and its significance in capturing the importance of words across documents. Compare the results of TF-IDF with the BoW approach.\n",
    "\n",
    "One-Hot Encoding: Investigate the application of One-Hot Encoding to encode categorical variables or labels. Can One-Hot Encoding be used directly for text classification? Why or why not?\n",
    "\n",
    "Deliverables:\n",
    "\n",
    "Present insights gathered from data exploration and discuss the impact of different feature engineering techniques (BoW, N-grams, TF-IDF, One-Hot Encoding). Provide recommendations for the best feature engineering strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22fc8835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   publish_date                                      headline_text\n",
      "0      20030219  aba decides against community broadcasting lic...\n",
      "1      20030219     act fire witnesses must be aware of defamation\n",
      "2      20030219     a g calls for infrastructure protection summit\n",
      "3      20030219           air nz staff in aust strike for pay rise\n",
      "4      20030219      air nz strike to affect australian travellers\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"abcnews-date-text1.csv\")\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(data.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f0563f",
   "metadata": {},
   "source": [
    "# Bag-of-Words (BoW):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d058b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (BoW - Unigram): 1.0\n",
      "Accuracy (BoW - Bigram): 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['headline_text'], data['publish_date'], test_size=0.2, random_state=42)\n",
    "\n",
    "# BoW with unigrams\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "X_test_bow = vectorizer.transform(X_test)\n",
    "\n",
    "# BoW with bigrams\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "X_train_bigram = bigram_vectorizer.fit_transform(X_train)\n",
    "X_test_bigram = bigram_vectorizer.transform(X_test)\n",
    "\n",
    "# Train a classifier (e.g., Naive Bayes) and evaluate accuracy\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_bow, y_train)\n",
    "y_pred = clf.predict(X_test_bow)\n",
    "accuracy_bow = accuracy_score(y_test, y_pred)\n",
    "\n",
    "clf.fit(X_train_bigram, y_train)\n",
    "y_pred_bigram = clf.predict(X_test_bigram)\n",
    "accuracy_bigram = accuracy_score(y_test, y_pred_bigram)\n",
    "\n",
    "# Compare accuracy of unigram and bigram BoW\n",
    "print(f\"Accuracy (BoW - Unigram): {accuracy_bow}\")\n",
    "print(f\"Accuracy (BoW - Bigram): {accuracy_bigram}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32850516",
   "metadata": {},
   "source": [
    "# Advantages and Limitations of BoW:\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Simple and easy to implement.\n",
    "Captures the presence of words in a document.\n",
    "Works well for tasks where word order is not crucial.\n",
    "\n",
    "\n",
    "Limitations:\n",
    "\n",
    "Ignores word semantics and context.\n",
    "Treats each word as independent, losing the sequence information.\n",
    "Sensitive to stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3149034c",
   "metadata": {},
   "source": [
    "# N-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ecf31388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (BoW - Unigram): 1.0\n",
      "Accuracy (BoW - Bigram): 1.0\n",
      "Accuracy (BoW - Trigram): 1.0\n"
     ]
    }
   ],
   "source": [
    "# N-grams (bi-grams, tri-grams)\n",
    "trigram_vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
    "X_train_trigram = trigram_vectorizer.fit_transform(X_train)\n",
    "X_test_trigram = trigram_vectorizer.transform(X_test)\n",
    "\n",
    "# Train a classifier and evaluate accuracy\n",
    "clf.fit(X_train_trigram, y_train)\n",
    "y_pred_trigram = clf.predict(X_test_trigram)\n",
    "accuracy_trigram = accuracy_score(y_test, y_pred_trigram)\n",
    "\n",
    "# Compare accuracy of unigram, bigram, and trigram BoW\n",
    "print(f\"Accuracy (BoW - Unigram): {accuracy_bow}\")\n",
    "print(f\"Accuracy (BoW - Bigram): {accuracy_bigram}\")\n",
    "print(f\"Accuracy (BoW - Trigram): {accuracy_trigram}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f83c42b",
   "metadata": {},
   "source": [
    "# TF-IDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "079f9a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (BoW): 1.0\n",
      "Accuracy (TF-IDF): 1.0\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Train a classifier and evaluate accuracy\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "y_pred_tfidf = clf.predict(X_test_tfidf)\n",
    "accuracy_tfidf = accuracy_score(y_test, y_pred_tfidf)\n",
    "\n",
    "# Compare accuracy of BoW and TF-IDF\n",
    "print(f\"Accuracy (BoW): {accuracy_bow}\")\n",
    "print(f\"Accuracy (TF-IDF): {accuracy_tfidf}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4cc30889",
   "metadata": {},
   "source": [
    "How TF-IDF Works:\n",
    "\n",
    "TF-IDF assigns weights to words based on their frequency in a document and across all documents. It gives higher weights to words that are frequent in a document but rare in other documents.\n",
    "\n",
    "One-Hot Encoding:\n",
    "\n",
    "Investigate the application of One-Hot Encoding.\n",
    "\n",
    "One-Hot Encoding is typically not used directly for text classification because it creates a binary vector for each word, resulting in high-dimensional and sparse data. It does not consider word semantics or their relationships.\n",
    "\n",
    "Summary and Recommendations:\n",
    "\n",
    "BoW vs. TF-IDF:\n",
    "\n",
    "BoW is simple and captures word presence, but it may not capture semantics.\n",
    "TF-IDF considers word importance across documents, capturing more meaningful information.\n",
    "N-grams:\n",
    "\n",
    "Adding N-grams (bi-grams, tri-grams) can capture some sequential information, improving accuracy.\n",
    "Recommendations:\n",
    "\n",
    "For a balance of simplicity and performance, use TF-IDF.\n",
    "Experiment with N-grams to capture sequential information.\n",
    "Consider advanced techniques like word embeddings for better semantic understanding."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
